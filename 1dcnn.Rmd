---
title: An R Markdown document converted from "softorder1dcnn/softordering1dcnn-on-sctp.ipynb"
output: html_document
---

# SoftOrdering1DCNN on Sbinary (2 class) NVC problem

In this notebook we will apply the neural network architecture winner of the 2nd place on MoA (https://www.kaggle.com/c/lish-moa/discussion/202256) to the Santander customer transaction prediction problem.

The results of this network (which we call SoftOrdering1DCNN) are benchmarked against a plain MLP and LightGBM.

Also info from this site https://medium.com/spikelab/convolutional-neural-networks-on-tabular-datasets-part-1-4abdd67795b6


```{r}
library(reticulate)
use_condaenv("torch")
```


```{python}
import numpy as np
import pandas as pd
from sklearn.metrics import roc_auc_score
import lightgbm as lgb

import torch
from torch import nn
from torch.utils.data import DataLoader,TensorDataset
from torch.optim.lr_scheduler import ReduceLROnPlateau
# added as model.summarize() gave an error
from torchvision import models
# from torchsummary import summary


import pytorch_lightning as pl
from pytorch_lightning.callbacks.early_stopping import EarlyStopping
```

```{python}
# needed for deterministic output
SEED = 2
np.random.seed(SEED)
torch.manual_seed(SEED)
torch.cuda.manual_seed(SEED)
torch.cuda.manual_seed_all(SEED)
torch.backends.cudnn.deterministic = True

# device in which the model will be trained
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
device
```
## data preparation

```{python}
dataset = pd.read_csv("data/pseudos.csv")
dataset
```

```{python}
# dataset split: train 60% - valid 20% - test 20%

index = np.array(dataset.index)
np.random.shuffle(index)
n = len(index)

train_index = index[0:int(0.6*n)]
valid_index = index[int(0.6*n):int(0.8*n)]
test_index = index[int(0.8*n):]

train_dset = dataset.loc[train_index].reset_index(drop=True)
valid_dset = dataset.loc[valid_index].reset_index(drop=True)
test_dset = dataset.loc[test_index].reset_index(drop=True)

input_features = dataset.columns[2:].tolist()
target = "target"
```

```{python}
# parsing inputs as pytorch tensor dataset

train_tensor_dset = TensorDataset(
    torch.tensor(train_dset[input_features].values, dtype=torch.float),
    #torch.tensor(train_dset[target].values.reshape(-1,1), dtype=torch.float)
    torch.tensor(train_dset[target].values, dtype=torch.long)
    #torch.tensor(train_dset[target].values.reshape(-1,3), dtype=torch.float)
    #torch.tensor(train_dset[target].values.reshape(-1), dtype=torch.long)
)

valid_tensor_dset = TensorDataset(
    torch.tensor(valid_dset[input_features].values, dtype=torch.float),
    #torch.tensor(valid_dset[target].values.reshape(-1,1), dtype=torch.float)
    torch.tensor(valid_dset[target].values, dtype=torch.long)
    #torch.tensor(train_dset[target].values.reshape(-1,3), dtype=torch.float)
    #torch.tensor(train_dset[target].values.reshape(-1), dtype=torch.long)
)

test_tensor_dset = TensorDataset(
    torch.tensor(test_dset[input_features].values, dtype=torch.float),
    #torch.tensor(test_dset[target].values.reshape(-1,1), dtype=torch.float) 
    torch.tensor(test_dset[target].values, dtype=torch.long) 
    #torch.tensor(train_dset[target].values.reshape(-1,3), dtype=torch.float)
    #torch.tensor(train_dset[target].values.reshape(-1), dtype=torch.long)
)
```

## SoftOrdering1DCNN
Parameters used:

* input_dim: the number of features at input.
* output_dim: the number of target values to fit.
* sign_size: the size of the signals to feed the first convolutional layer.
* cha_input: number of channels to feed the first convolutional layer.
* cha_hidden: number of channels computed by the hidden convolutional layers.
* K: channel increase rate for the first convolutional layer. The first layer will increase channels from cha_input to K*cha_input.
* dropout_input: dropout rate to apply at the input.
* dropout_hidden: dropout rate to apply at the hidden convolutional layers.
* dropout_output: dropout rate to apply in the last fully connected layer.

```{python}
class SoftOrdering1DCNN(pl.LightningModule):

    def __init__(self, input_dim, output_dim, sign_size=32, cha_input=16, cha_hidden=32, 
                 K=2, dropout_input=0.2, dropout_hidden=0.2, dropout_output=0.2):
        super().__init__()

        hidden_size = sign_size*cha_input
        sign_size1 = sign_size
        sign_size2 = sign_size//2
        output_size = (sign_size//4) * cha_hidden

        self.hidden_size = hidden_size
        self.cha_input = cha_input
        self.cha_hidden = cha_hidden
        self.K = K
        self.sign_size1 = sign_size1
        self.sign_size2 = sign_size2
        self.output_size = output_size
        self.dropout_input = dropout_input
        self.dropout_hidden = dropout_hidden
        self.dropout_output = dropout_output

        self.batch_norm1 = nn.BatchNorm1d(input_dim)
        self.dropout1 = nn.Dropout(dropout_input)
        dense1 = nn.Linear(input_dim, hidden_size, bias=False)
        self.dense1 = nn.utils.weight_norm(dense1)

        # 1st conv layer
        self.batch_norm_c1 = nn.BatchNorm1d(cha_input)
        conv1 = conv1 = nn.Conv1d(
            cha_input, 
            cha_input*K, 
            kernel_size=5, 
            stride = 1, 
            padding=2,  
            groups=cha_input, 
            bias=False)
        self.conv1 = nn.utils.weight_norm(conv1, dim=None)

        self.ave_po_c1 = nn.AdaptiveAvgPool1d(output_size = sign_size2)

        # 2nd conv layer
        self.batch_norm_c2 = nn.BatchNorm1d(cha_input*K)
        self.dropout_c2 = nn.Dropout(dropout_hidden)
        conv2 = nn.Conv1d(
            cha_input*K, 
            cha_hidden, 
            kernel_size=3, 
            stride=1, 
            padding=1, 
            bias=False)
        self.conv2 = nn.utils.weight_norm(conv2, dim=None)

        # 3rd conv layer
        self.batch_norm_c3 = nn.BatchNorm1d(cha_hidden)
        self.dropout_c3 = nn.Dropout(dropout_hidden)
        conv3 = nn.Conv1d(
            cha_hidden, 
            cha_hidden, 
            kernel_size=3, 
            stride=1, 
            padding=1, 
            bias=False)
        self.conv3 = nn.utils.weight_norm(conv3, dim=None)
        

        # 4th conv layer
        self.batch_norm_c4 = nn.BatchNorm1d(cha_hidden)
        conv4 = nn.Conv1d(
            cha_hidden, 
            cha_hidden, 
            kernel_size=5, 
            stride=1, 
            padding=2, 
            groups=cha_hidden, 
            bias=False)
        self.conv4 = nn.utils.weight_norm(conv4, dim=None)

        self.avg_po_c4 = nn.AvgPool1d(kernel_size=4, stride=2, padding=1)

        self.flt = nn.Flatten()

        self.batch_norm2 = nn.BatchNorm1d(output_size)
        self.dropout2 = nn.Dropout(dropout_output)
        dense2 = nn.Linear(output_size, output_dim, bias=False)
        self.dense2 = nn.utils.weight_norm(dense2)

        #self.loss = nn.BCEWithLogitsLoss()
        self.loss = nn.CrossEntropyLoss()

    def forward(self, x):
        x = self.batch_norm1(x)
        x = self.dropout1(x)
        x = nn.functional.celu(self.dense1(x))

        x = x.reshape(x.shape[0], self.cha_input, self.sign_size1)

        x = self.batch_norm_c1(x)
        x = nn.functional.relu(self.conv1(x))

        x = self.ave_po_c1(x)

        x = self.batch_norm_c2(x)
        x = self.dropout_c2(x)
        x = nn.functional.relu(self.conv2(x))
        x_s = x

        x = self.batch_norm_c3(x)
        x = self.dropout_c3(x)
        x = nn.functional.relu(self.conv3(x))

        x = self.batch_norm_c4(x)
        x = self.conv4(x)
        x =  x + x_s
        x = nn.functional.relu(x)

        x = self.avg_po_c4(x)

        x = self.flt(x)

        x = self.batch_norm2(x)
        x = self.dropout2(x)
        x = self.dense2(x)

        return x

    def training_step(self, batch, batch_idx):
        X, y = batch
        y_hat = self.forward(X)
        loss = self.loss(y_hat, y)
        self.log('train_loss', loss)
        return loss
    
    def validation_step(self, batch, batch_idx):
        X, y = batch
        y_hat = self.forward(X)
        loss = self.loss(y_hat, y)
        self.log('valid_loss', loss)
        
    def test_step(self, batch, batch_idx):
        X, y = batch
        y_logit = self.forward(X)
        y_probs = torch.sigmoid(y_logit).detach().cpu().numpy()
        loss = self.loss(y_logit, y)
        metric = roc_auc_score(y.cpu().numpy(), y_probs)
        self.log('test_loss', loss)
        self.log('test_metric', metric)
        
    def configure_optimizers(self):
        optimizer = torch.optim.SGD(self.parameters(), lr=1e-2, momentum=0.9)
        scheduler = {
            'scheduler': ReduceLROnPlateau(
                optimizer, 
                mode="min", 
                factor=0.5, 
                patience=5, 
                min_lr=1e-5),
            'interval': 'epoch',
            'frequency': 1,
            'reduce_on_plateau': True,
            'monitor': 'valid_loss',
        }
        return [optimizer], [scheduler]
```

```{python}
model = SoftOrdering1DCNN(
    input_dim=len(input_features), 
    output_dim=3, # Originally 1 (for 2 binary outputs); change to 2 for 3 ?
    sign_size=16, 
    cha_input=64, 
    cha_hidden=64, 
    K=2, 
    dropout_input=0.3, 
    dropout_hidden=0.3, 
    dropout_output=0.2
)

early_stop_callback = EarlyStopping(
   monitor='valid_loss',
   min_delta=.0,
   patience=21,
   verbose=True,
   mode='min'
)

trainer = pl.Trainer(callbacks=[early_stop_callback], min_epochs=10, max_epochs=5)
```

```{python}
model.summarize()
```

```{python}
trainer.fit(
    model, 
    DataLoader(train_tensor_dset, batch_size=2048, shuffle=True, num_workers=4),
    DataLoader(valid_tensor_dset, batch_size=2048, shuffle=False, num_workers=4)
)
```

```{python}
# AUC on validation dataset
trainer.test(model, DataLoader(valid_tensor_dset, batch_size=2048, shuffle=False, num_workers=4))
```

```{python}
# AUC on test dataset
trainer.test(model, DataLoader(test_tensor_dset, batch_size=2048, shuffle=False, num_workers=4))
```

```{python}
y_pred_proba = []
for batch in DataLoader(test_tensor_dset, batch_size=2048, shuffle=False, num_workers=4):
    inputs = batch[0].to(device)
    outputs = model(inputs)
    y_pred_proba.append(outputs.detach().cpu().numpy())
y_pred_proba = np.concatenate(y_pred_proba)


# Get the true labels for the test data
y_true = test_tensor_dset.tensors[1].numpy()

# Calculate the ROC AUC score
roc_auc = roc_auc_score(y_true, y_pred_proba, multi_class='ovr')

```

